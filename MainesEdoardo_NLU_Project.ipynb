{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# **Natural Language Understanding Final Project**\n","### Edoardo Maines (232226)\n","\n","**Description**: implementation of a Language Model using one of the RNN architectures (eg. Vanilla, LSTM, GRU)\n","\n","**Dataset**: Penn Treebank\n","\n","**Main reference papers**: \n","\n","- Zaremba, Wojciech, et al. _Recurrent Neural Network Regularization._ 1 Sept. 2014. NASA ADS, https://ui.adsabs.harvard.edu/abs/2014arXiv1409.2329Z.\n","\n","- Gal, Yarin, and Zoubin Ghahramani. _A Theoretically Grounded Application of Dropout in Recurrent Neural Networks._ arXiv:1512.05287, arXiv, 5 Oct. 2016. arXiv.org, http://arxiv.org/abs/1512.05287.\n","\n","**Acknowledgement**: since this is one of the first times I try to implement deep neural architectures I needed some help to get started. That's why I used some GitHub repositories to first understand the usage of the main functions. For some small parts of the code I also took inspiration from them but the majority of the code was developed by me. \n","\n","Here are the repositories that I investigated the most to understand things:\n","- [`floydhub/word-language-model`](https://github.com/floydhub/word-language-model/tree/19eac43f43bdb415aa8b23d9827ccd410f15e545)\n","- [`ahmetumutdurmus/zaremba`](https://github.com/ahmetumutdurmus/zaremba)\n","- [`wojzaremba/lstm`](https://github.com/ahmetumutdurmus/zaremba)\n","- [`PetrochukM/PyTorch-NLP`](https://github.com/PetrochukM/PyTorch-NLP)\n","\n","\n"],"metadata":{"id":"Qk5YeGhViddB"}},{"cell_type":"markdown","source":["I will use [PyTorch-NLP](https://pytorchnlp.readthedocs.io/en/latest/index.html) to retrieve and easily manipulate the Penn Treebank dataset. However, due to some lack of compatibility between the latest version of PyTorch (1.12.0+cu113) and PyTorch-NLP `BPTTBatchSampler`, I installed an older version of PyTorch (1.8.0+cu111).\n","\n","Sometimes at the end of the installation it is asked to restart runtime. After the restarting, the correct version of PyTorch will be there."],"metadata":{"id":"4lU6R1h3x9JC"}},{"cell_type":"code","execution_count":24,"metadata":{"id":"K5Y0V75VxrCC","colab":{"base_uri":"https://localhost:8080/"},"outputId":"d66423b1-984e-401d-8f33-0c9e6cb48ae0","executionInfo":{"status":"ok","timestamp":1675422247796,"user_tz":-60,"elapsed":9414,"user":{"displayName":"Edoardo Maines","userId":"11184902831171502919"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Looking in links: https://download.pytorch.org/whl/torch_stable.html\n","Requirement already satisfied: torch==1.8.0+cu111 in /usr/local/lib/python3.8/dist-packages (1.8.0+cu111)\n","Requirement already satisfied: torchvision==0.9.0+cu111 in /usr/local/lib/python3.8/dist-packages (0.9.0+cu111)\n","Requirement already satisfied: torchaudio==0.8.0 in /usr/local/lib/python3.8/dist-packages (0.8.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch==1.8.0+cu111) (4.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torch==1.8.0+cu111) (1.21.6)\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.8/dist-packages (from torchvision==0.9.0+cu111) (7.1.2)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: pytorch-nlp in /usr/local/lib/python3.8/dist-packages (0.5.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from pytorch-nlp) (1.21.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from pytorch-nlp) (4.64.1)\n"]}],"source":["# Install Pytorch-NLP and the PyTorch version compatible with 'BPTTBatchSampler'\n","!pip install torch==1.8.0+cu111 torchvision==0.9.0+cu111 torchaudio==0.8.0 -f https://download.pytorch.org/whl/torch_stable.html\n","!pip install pytorch-nlp"]},{"cell_type":"code","source":["print(\"Importing required libraries...\")\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","try:\n","    from torchnlp.datasets import penn_treebank_dataset  \n","    from torchnlp.samplers import BPTTBatchSampler\n","    from torchnlp.encoders import LabelEncoder\n","    from torchnlp.nn import LockedDropout\n","except:\n","    if torch.__version__ != \"1.8.0+cu111\":\n","        print(\"You don't have the correct version of PyTorch! Please install version 1.8.0+cu111 using commad above.\")\n","\n","import math\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from collections import Counter\n","from tqdm import tqdm\n","\n","\n","print(\"All packages correctly loaded!\")\n","\n","print(\"PyTorch version \", torch.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g0wjycY0yW8H","outputId":"f42615cf-f363-4279-e51b-4da21bb4d5a4","executionInfo":{"status":"ok","timestamp":1675422247800,"user_tz":-60,"elapsed":59,"user":{"displayName":"Edoardo Maines","userId":"11184902831171502919"}}},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Importing required libraries...\n","All packages correctly loaded!\n","PyTorch version  1.8.0+cu111\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# change the directory to the folder where the notebook is placed\n","%cd /content/drive/MyDrive/Colab Notebooks/NLUproj2022\n","! ls"],"metadata":{"id":"GjcXB7ip0N11","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6c99bc76-72fc-414e-c517-ac670d091252","executionInfo":{"status":"ok","timestamp":1675422253659,"user_tz":-60,"elapsed":5900,"user":{"displayName":"Edoardo Maines","userId":"11184902831171502919"}}},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive/Colab Notebooks/NLUproj2022\n","data\t\t MainesEdoardo_NLU_Project.ipynb  noREGout.txt\t  varREGout.txt\n","largeREGout.txt  medREGout.txt\t\t\t  trained_models\n"]}]},{"cell_type":"markdown","source":["## **Dataset Analysis**\n","I now load and analyze the Penn Treebank dataset [[1](https://aclanthology.org/J93-2004/)]\n","\n","NB: \n","in the dataset there are numerous symbols due to pre-processing:\n","*  while loading the dataset, the tag '**<\\s>**' is added to the end of each sentence.\n","*  every number has been replaced by '**N**' and every word outside the dictionary by '**\\<unk\\>**'\n","\n","So, in these analyses, we will see the following cases:\n","\n","*   with and without '**<\\s>**'\n","*   with and without '**<\\s>**', '**N**' and '**\\<unk\\>**'\n","\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"8ulxH5CL2MwF"}},{"cell_type":"code","source":["# (PyTorch-NLP) 'penn_treebank_dataset' loads the Penn Treebank Project: \n","# Release 2 CDROM, featuring a million words of 1989 Wall Street Journal material.\n","\n","train, val, test = penn_treebank_dataset(train=True, dev = True, test = True)\n","\n","corpus = train + val + test\n","# (PyTorch-NLP) 'LabelEncoder' encodes labels via dictionary. It will contain the \n","# vocabulary and useful methods to convert text into labels and vice versa\n","\n","encoder = LabelEncoder(train, reserved_labels=[])\n","\n","# function to convert list into set, in order to have the length of the single vocabolary\n","val_vocab = set(val)\n","test_vocab = set(test)\n","print(corpus[:30])\n","\n","print(\"Percentage of training set on the total is {:.1f}%\".format((len(train)/len(corpus))*100))\n","print(\"Percentage of validation set on the total is {:.1f}%\".format((len(val)/len(corpus))*100))\n","print(\"Percentage of testing set on the total is {:.1f}%\".format((len(test)/len(corpus))*100))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V4F5pMj62MEW","outputId":"166fb179-e47a-4985-8d95-74feba41d5c0","executionInfo":{"status":"ok","timestamp":1675422254441,"user_tz":-60,"elapsed":807,"user":{"displayName":"Edoardo Maines","userId":"11184902831171502919"}}},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["['aer', 'banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano', 'guterman', 'hydro-quebec', 'ipo', 'kia', 'memotec', 'mlx', 'nahb', 'punts', 'rake', 'regatta', 'rubens', 'sim', 'snack-food', 'ssangyong', 'swapo', 'wachter', '</s>', 'pierre', '<unk>', 'N', 'years', 'old']\n","Percentage of training set on the total is 85.6%\n","Percentage of validation set on the total is 6.8%\n","Percentage of testing set on the total is 7.6%\n"]}]},{"cell_type":"markdown","source":["Now, I analyze the sentences present in each dataset (Train, Validation and Test)\n","\n","I calculated the length of the minor sentence, the major sentence and the average sentence.\n","\n","NB: We don't want to count also '**<\\s>**'"],"metadata":{"id":"cUPb71SizMhM"}},{"cell_type":"code","source":["train_sents_len = np.empty([0])\n","val_sents_len = np.empty([0])\n","test_sents_len = np.empty([0])\n","\n","counter = 0\n","\n","#TRAIN\n","for i in train:\n","    if i != \"</s>\":\n","        counter += 1\n","    else:\n","        train_sents_len = np.append(train_sents_len, counter)\n","        counter = 0\n","\n","#VALIDATION\n","for i in val:\n","    if i != \"</s>\":\n","        counter += 1\n","    else:\n","        val_sents_len = np.append(val_sents_len, counter)\n","        counter = 0\n","\n","#TEST\n","for i in test:\n","    if i != \"</s>\":\n","        counter += 1\n","    else:\n","        test_sents_len = np.append(test_sents_len, counter)\n","        counter = 0\n","\n","\n","# Min, Max, AVG in Train\n","train_min_sent_len = np.min(train_sents_len)\n","train_max_sent_len = np.max(train_sents_len)\n","train_avg_sent_len = np.mean(train_sents_len)\n","\n","# Min, Max, AVG in Validation\n","val_min_sent_len = np.min(val_sents_len)\n","val_max_sent_len = np.max(val_sents_len)\n","val_avg_sent_len = np.mean(val_sents_len)\n","\n","# Min, Max, AVG in Test\n","test_min_sent_len = np.min(test_sents_len)\n","test_max_sent_len = np.max(test_sents_len)\n","test_avg_sent_len = np.mean(test_sents_len)\n","\n","\n","print(\"\\n|Dataset   |MIN |MAX |AVG |\")\n","print(\"===========================\")\n","print(\"|Train     | {:.0f} | {:.0f} | {:.0f} |\".format(train_min_sent_len, train_max_sent_len, train_avg_sent_len))\n","print(\"|Validation| {:.0f} | {:.0f} | {:.0f} |\".format(val_min_sent_len, val_max_sent_len, val_avg_sent_len))\n","print(\"|Test      | {:.0f} | {:.0f} | {:.0f} |\".format(test_min_sent_len, test_max_sent_len, test_avg_sent_len))\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t5QT7uzjXkNF","outputId":"a88a56d1-843e-4236-af8d-89dd4b1addc6","executionInfo":{"status":"ok","timestamp":1675422254442,"user_tz":-60,"elapsed":25,"user":{"displayName":"Edoardo Maines","userId":"11184902831171502919"}}},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","|Dataset   |MIN |MAX |AVG |\n","===========================\n","|Train     | 1 | 82 | 21 |\n","|Validation| 1 | 74 | 21 |\n","|Test      | 1 | 77 | 21 |\n"]}]},{"cell_type":"markdown","source":["A random example taken from the training PTB set shows us that sentences are already heavily pre-processed."],"metadata":{"id":"EixxTm0B6LhU"}},{"cell_type":"code","source":["\" \".join(train[41:53])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"4p76lgyZ5Rox","outputId":"1c66fa08-6b88-4e8f-e09b-573321028c7c","executionInfo":{"status":"ok","timestamp":1675422254443,"user_tz":-60,"elapsed":14,"user":{"displayName":"Edoardo Maines","userId":"11184902831171502919"}}},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'mr. <unk> is chairman of <unk> n.v. the dutch publishing group </s>'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","source":["Now, the datased will be cleaned and we'll have 2 version:"],"metadata":{"id":"s-I7ccFI93Dl"}},{"cell_type":"code","source":["# we first remove only the </s> symbol\n","train_clean_1 = [word for word in train if word != \"</s>\"]\n","val_clean_1 = [word for word in val if word != \"</s>\"]\n","test_clean_1 = [word for word in test if word != \"</s>\"]\n","\n","corpus_clean_1 = train_clean_1 + val_clean_1 + test_clean_1\n","\n","train_clean_1_voc = set(train_clean_1)\n","val_clean_1_voc = set(val_clean_1)\n","test_clean_1_voc = set(test_clean_1)\n","\n","# now,  we remove all the symbols used for processing the corpus (</s>, <unk>, N)\n","train_clean_2 = [word for word in train if word != \"</s>\" and word != \"<unk>\" and word != \"N\"]\n","val_clean_2 = [word for word in val if word != \"</s>\" and word != \"<unk>\" and word != \"N\"]\n","test_clean_2 = [word for word in test if word != \"</s>\" and word != \"<unk>\" and word != \"N\"]\n","\n","corpus_clean_2 = train_clean_2 + val_clean_2 + test_clean_2\n","\n","train_clean_2_voc = set(train_clean_2)\n","val_clean_2_voc = set(val_clean_2)\n","test_clean_2_voc = set(test_clean_2)"],"metadata":{"id":"rUhNUglO83SR","executionInfo":{"status":"ok","timestamp":1675422254873,"user_tz":-60,"elapsed":440,"user":{"displayName":"Edoardo Maines","userId":"11184902831171502919"}}},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":["At this point, we have, for every dataset:\n","\n","\n","*   complete version:\n","  *   *train*;\n","  *   *val*;\n","  *   *test*.\n","*   without <\\s>:\n","  *   *train_clean_1*;\n","  *   *val_clean_1*;\n","  *   *test_clean_1*.\n","*   without <\\s>, \\<unk\\>, N:\n","  *   *train_clean_2*;\n","  *   *val_clean_2*;\n","  *   *test_clean_2*.\n","\n","Let's see, for these sets, the **number of sentences**, the **vocabolary size** and the **number of words**.\n","\n","NB: for the number of sentences, we will use always the same variables because it won't change with or without the symbols.\n","\n","\n","\n"],"metadata":{"id":"CHnxilWAJV0h"}},{"cell_type":"code","source":["print(\"\\n\\n Dataset with <\\s>, <unk>, 'N'\")\n","print(\"\\n|Dataset   |#Sentences |Vocab_size |#Words|\")\n","print(\"=========================================\")\n","print(\"|Train     | {:.0f} | {:.0f} | {:.0f} |\".format(len(train_sents_len), encoder.vocab_size, len(train)))\n","print(\"|Validation| {:.0f} | {:.0f} | {:.0f} |\".format(len(val_sents_len), len(val_vocab), len(val)))\n","print(\"|Test      | {:.0f} | {:.0f} | {:.0f} |\".format(len(test_sents_len), len(test_vocab), len(test)))\n","\n","print(\"\\n\\n Dataset with <unk>, 'N'\")\n","print(\"\\n|Dataset   |#Sentences |Vocab_size |#Words|\")\n","print(\"=========================================\")\n","print(\"|Train     | {:.0f} | {:.0f} | {:.0f} |\".format(len(train_sents_len), len(train_clean_1_voc), len(train_clean_1)))\n","print(\"|Validation| {:.0f} | {:.0f} | {:.0f} |\".format(len(val_sents_len), len(val_clean_1_voc), len(val_clean_1)))\n","print(\"|Test      | {:.0f} | {:.0f} | {:.0f} |\".format(len(test_sents_len), len(test_clean_1_voc), len(test_clean_1)))\n","\n","print(\"\\n\\n Dataset without <\\s>, <unk>, 'N'\")\n","print(\"\\n|Dataset   |#Sentences |Vocab_size |#Words|\")\n","print(\"=========================================\")\n","print(\"|Train     | {:.0f} | {:.0f} | {:.0f} |\".format(len(train_sents_len), len(train_clean_2_voc), len(train_clean_2)))\n","print(\"|Validation| {:.0f} | {:.0f} | {:.0f} |\".format(len(val_sents_len), len(val_clean_2_voc), len(val_clean_2)))\n","print(\"|Test      | {:.0f} | {:.0f} | {:.0f} |\".format(len(test_sents_len), len(test_clean_2_voc), len(test_clean_2)))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zY7JNmE6IAtf","outputId":"0723c626-34e8-416d-efbc-7b2d1af93cff","executionInfo":{"status":"ok","timestamp":1675422254875,"user_tz":-60,"elapsed":31,"user":{"displayName":"Edoardo Maines","userId":"11184902831171502919"}}},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n"," Dataset with <\\s>, <unk>, 'N'\n","\n","|Dataset   |#Sentences |Vocab_size |#Words|\n","=========================================\n","|Train     | 42068 | 10000 | 929589 |\n","|Validation| 3370 | 6022 | 73760 |\n","|Test      | 3761 | 6049 | 82430 |\n","\n","\n"," Dataset with <unk>, 'N'\n","\n","|Dataset   |#Sentences |Vocab_size |#Words|\n","=========================================\n","|Train     | 42068 | 9999 | 887521 |\n","|Validation| 3370 | 6021 | 70390 |\n","|Test      | 3761 | 6048 | 78669 |\n","\n","\n"," Dataset without <\\s>, <unk>, 'N'\n","\n","|Dataset   |#Sentences |Vocab_size |#Words|\n","=========================================\n","|Train     | 42068 | 9997 | 810020 |\n","|Validation| 3370 | 6019 | 64302 |\n","|Test      | 3761 | 6046 | 71352 |\n"]}]},{"cell_type":"code","source":["# let's define a function to represent the most 10 frequent words in the input corpus\n","def ten_most_freq(corpus):\n","\n","  freq_list = Counter(corpus)\n","\n","  x = []\n","  y = []\n","\n","  k_all = np.empty(len(freq_list))\n","\n","  display = 10\n","  print(\"\\n|Rank|Freq|Word|\")\n","  print(\"================\")\n","  for i, word in enumerate(freq_list.most_common(len(freq_list))):\n","      x.append(i+1)\n","      y.append(word[1]/len(corpus))\n","      k_all[i] = (i+1)*(word[1]/len(corpus))\n","      if display > 0:\n","          print(\"|{:d}|{:d}|{}|\".format(i+1, word[1], word[0]))\n","          display -= 1\n","\n","  return x, y, k_all"],"metadata":{"id":"IDffAjEL626k","executionInfo":{"status":"ok","timestamp":1675422254877,"user_tz":-60,"elapsed":22,"user":{"displayName":"Edoardo Maines","userId":"11184902831171502919"}}},"execution_count":32,"outputs":[]},{"cell_type":"code","source":["print(\"\\n\\nMost 10 frequent tokens (with '</s>', '<unk>' and 'N')\")\n","x_1, y_1, k_all_1 = ten_most_freq(corpus_clean_1)\n","\n","print(\"\\n\\nMost 10 frequent tokens (without '</s>', '<unk>' and 'N')\")\n","x_2, y_2, k_all_2 = ten_most_freq(corpus_clean_2)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DOAgJIHo49go","outputId":"39da8a61-1f1f-4549-87a8-d66c6f3c1cab","executionInfo":{"status":"ok","timestamp":1675422255548,"user_tz":-60,"elapsed":690,"user":{"displayName":"Edoardo Maines","userId":"11184902831171502919"}}},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","Most 10 frequent tokens (with '</s>', '<unk>' and 'N')\n","\n","|Rank|Freq|Word|\n","================\n","|1|59421|the|\n","|2|53299|<unk>|\n","|3|37607|N|\n","|4|28427|of|\n","|5|27430|to|\n","|6|24755|a|\n","|7|21032|in|\n","|8|20404|and|\n","|9|11555|'s|\n","|10|10436|for|\n","\n","\n","Most 10 frequent tokens (without '</s>', '<unk>' and 'N')\n","\n","|Rank|Freq|Word|\n","================\n","|1|59421|the|\n","|2|28427|of|\n","|3|27430|to|\n","|4|24755|a|\n","|5|21032|in|\n","|6|20404|and|\n","|7|11555|'s|\n","|8|10436|for|\n","|9|10419|that|\n","|10|8764|$|\n"]}]},{"cell_type":"markdown","source":["## **Model Architecture**\n","\n","We now define the RNN model architecture with LSTM units used in this project that follows *Zaremba et al.* [[2](https://arxiv.org/abs/1409.2329)]\n","\n","Architecture overview:\n","\n","\n","```\n","Architecture(\n","  (emb): Embedding(vocab_size, custom_emb_dim)\n","  (rnn): LSTM(custom_emb_dim, custom_hidden_size, num_layers= custom_nLayers)\n","  (decoder): Linear(in_features=custom_hidden_size, out_features=vocab_size, bias=True)\n",")\n","```\n","\n","This is the base for all the NNs used in this project. Depending on the version we are dealing with dropout will be used in different ways and places. \n","\n","\n","\n"],"metadata":{"id":"Ro2m9d43_wNB"}},{"cell_type":"markdown","source":["In this first model implementation the dropout variable is unique and it will just store a probability of units drop. However, in this case the dropout will be applied on non-recurrent connections of the LSTM (as well as on generated embeddings and final output) in a naive manner.\n","\n","<img src=\"https://i.ibb.co/h8yrcHV/dropouts-Copy.png\" alt=\"dropouts-Copy\" border=\"0\" width = 400>\n","\n","_Image taken from Gal et al. paper_ [[3](http://arxiv.org/abs/1512.05287)]\n","\n"],"metadata":{"id":"8m-fELtsr-9d"}},{"cell_type":"code","source":["class Model(nn.Module):\n","\n","    def __init__(self, vocab_size, emb_size, hidden_size, n_layers, init_weight, dropout=0.5): \n","        super(Model, self).__init__()\n","        \n","        self.drop = nn.Dropout(dropout)\n","        self.emb = nn.Embedding(vocab_size, emb_size) \n","        self.rnn = nn.LSTM(emb_size, hidden_size, n_layers, dropout=dropout) \n","        self.decoder = nn.Linear(hidden_size, vocab_size)\n","        \n","        self.init_weight = init_weight\n","        self.hidden_size = hidden_size\n","        self.n_layers = n_layers\n","        \n","        self.init_weights()\n","        \n","    def init_weights(self):\n","        # Paper 4.1 ~ units of the layers are initialized uniformly in [-winit, winit] \n","        self.emb.weight.data.uniform_(-self.init_weight , self.init_weight )\n","        self.decoder.bias.data.fill_(0)\n","        self.decoder.weight.data.uniform_(-self.init_weight , self.init_weight )\n","\n","    def init_hidden(self, batch_size):\n","        # Paper 4.1 ~ initialize the hidden states to zero \n","        weight = next(self.parameters()).data\n","        return weight.new_zeros(self.n_layers, batch_size, self.hidden_size), weight.new_zeros(self.n_layers, batch_size, self.hidden_size)\n","\n","    def forward(self, input, hidden):\n","        embedded = self.drop(self.emb(input))\n","        output, hidden = self.rnn(embedded, hidden)\n","        output = self.drop(output)\n","        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n","        return decoded, hidden"],"metadata":{"id":"DzJt7r9jA4OD","executionInfo":{"status":"ok","timestamp":1675422255549,"user_tz":-60,"elapsed":25,"user":{"displayName":"Edoardo Maines","userId":"11184902831171502919"}}},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":["This is the second model implementation where I follow the idea of _Gal et al._ [[3](http://arxiv.org/abs/1512.05287)]\n","\n","<img src=\"https://i.ibb.co/4N1rTQn/dropouts-Copy-Copy.png\" alt=\"dropouts-Copy-Copy\" border=\"0\" width = 400>\n","\n","_Image taken from Gal et al. paper_ [[3](http://arxiv.org/abs/1512.05287)]\n","\n","In this case I tried to replicate exactly what we see in the picture. \n","\n","```\n","self.dropout_embedding = LockedDropout(dropout_embedding) # blue\n","self.dropout_rnnhid1 = LockedDropout(dropout_rnnhid) # orange\n","self.dropout_rnnhid2 = LockedDropout(dropout_rnnhid) # dark red\n","self.dropout_rnnlay = LockedDropout(dropout_rnnlay) # green\n","self.dropout_finalout = LockedDropout(dropout_finalout) # bright red\n","```\n","\n"],"metadata":{"id":"wvLHkVQvtQTu"}},{"cell_type":"code","source":["class VarModel(nn.Module):\n","\n","    def __init__(self, vocab_size, emb_size, hidden_size, n_layers, init_weight,\n","                 dropout_embedding=0.5,\n","                 dropout_lstm_hidden=0.5,\n","                 dropout_lstm_layer=0.5,\n","                 dropout_output=0.5\n","                 ): \n","\n","\n","        super(VarModel, self).__init__()\n","\n","        # define the layers\n","        self.emb = nn.Embedding(vocab_size, emb_size)\n","        self.lstm_1 = nn.LSTM(emb_size, hidden_size, 1, dropout=0) \n","        self.lstm_2 = nn.LSTM(emb_size, hidden_size, 1, dropout=0) \n","        self.decoder = nn.Linear(hidden_size, vocab_size)\n","\n","        # (PyTorch-NLP) 'LockedDropout' allows to create a dropout mask\n","        self.dropout_embedding = LockedDropout(dropout_embedding)\n","        self.dropout_lstm_hidden_1 = LockedDropout(dropout_lstm_hidden)\n","        self.dropout_lstm_hidden_2 = LockedDropout(dropout_lstm_hidden)\n","        self.dropout_lstm_layer = LockedDropout(dropout_lstm_layer)\n","        self.dropout_output = LockedDropout(dropout_output)\n","        \n","        self.decoder.weight = self.emb.weight\n","        \n","        self.init_weight = init_weight\n","        self.hidden_size = hidden_size\n","        self.n_layers = n_layers\n","        \n","        self.init_weights()\n","        \n","    def init_weights(self):\n","        self.emb.weight.data.uniform_(-self.init_weight , self.init_weight )\n","        self.decoder.bias.data.fill_(0)\n","        self.decoder.weight.data.uniform_(-self.init_weight , self.init_weight )\n","\n","    def init_hidden(self, batch_size):\n","        weight = next(self.parameters()).data\n","        return [(weight.new_zeros(1, batch_size, self.hidden_size, device = \"cuda:0\"),\n","                weight.new_zeros(1, batch_size, self.hidden_size, device = \"cuda:0\"))\n","                for l in range(self.n_layers)]\n","\n","    def forward(self, input, hidden):\n","        embedded = self.dropout_embedding(self.emb(input))\n","        new_hidden = []\n","\n","        out_1, h_1 = self.lstm_1(embedded, (self.dropout_lstm_hidden_1(hidden[0][0]), self.dropout_lstm_hidden_1(hidden[0][1])))\n","        out_1 = self.dropout_lstm_layer(out_1)\n","\n","        out_2, h_2 = self.lstm_2(out_1, (self.dropout_lstm_hidden_1(hidden[1][0]), self.dropout_lstm_hidden_1(hidden[1][1])))\n","        hidden = [h_1,h_2]\n","        \n","        output = self.dropout_output(out_2)\n","        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n","        return decoded, hidden"],"metadata":{"id":"Aax9n3_tv-Fb","executionInfo":{"status":"ok","timestamp":1675422255549,"user_tz":-60,"elapsed":24,"user":{"displayName":"Edoardo Maines","userId":"11184902831171502919"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["def repackage_hidden(h):\n","    if isinstance(h, torch.Tensor):\n","        return h.detach()\n","    else:\n","        return tuple(repackage_hidden(v) for v in h)"],"metadata":{"id":"irxutuPPEUXe","executionInfo":{"status":"ok","timestamp":1675422255550,"user_tz":-60,"elapsed":24,"user":{"displayName":"Edoardo Maines","userId":"11184902831171502919"}}},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":["## **Train and Test steps**"],"metadata":{"id":"LB6OzFUSEm2u"}},{"cell_type":"code","source":["def test_step(data_tensor, source_slices, target_slices):\n","    model.eval()\n","    total_loss = 0\n","    tot_samples = 0\n","\n","    with torch.no_grad():\n","        hidden = model.init_hidden(batch_size)\n","        \n","        for source_slice, target_slice in zip(source_slices, target_slices):\n","            data = torch.stack([data_tensor[i] for i in source_slice], dim = 1)\n","            targets = torch.stack([data_tensor[i] for i in target_slice], dim = 1).view(-1)\n","            \n","            output, hidden = model(data, hidden)\n","\n","            loss = criterion(output, targets) * output.size(0)\n","            total_loss += loss\n","            tot_samples += output.size(0)\n","\n","        avg_loss = total_loss/tot_samples\n","        avg_ppl = math.exp(avg_loss)\n","    return avg_loss, avg_ppl"],"metadata":{"id":"ITzNKR7cFLkA","executionInfo":{"status":"ok","timestamp":1675422255550,"user_tz":-60,"elapsed":23,"user":{"displayName":"Edoardo Maines","userId":"11184902831171502919"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["def train_step():\n","    model.train()\n","    hidden = model.init_hidden(batch_size)\n","    \n","    batch = 0\n","    if epoch >= epoch_decay_lr:\n","        scheduler.step() \n","\n","    \n","    for source_slice, target_slice in zip(train_source_sampler, train_target_sampler):\n","            \n","\n","        data = torch.stack([train_data[i] for i in source_slice], dim =1)\n","        targets = torch.stack([train_data[i] for i in target_slice], dim = 1).view(-1)\n","\n","        hidden = repackage_hidden(hidden)   \n","\n","        optimizer.zero_grad()\n","\n","        output, hidden = model(data, hidden)\n","\n","        loss = criterion(output, targets)\n","        loss.backward()\n","\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n","        optimizer.step()\n","        \n","        if batch%report_iterations == 0:\n","            print('| epoch {:d} | {:d}/{:d} batches | lr {:.3f} | loss {:.3f} |'.format(\n","                epoch+1, batch, len(train_source_sampler), scheduler.get_last_lr()[0], loss.item()\n","            ), file = f)\n","\n","        \n","        batch += 1\n"],"metadata":{"id":"mCTppzIWFN0M","executionInfo":{"status":"ok","timestamp":1675422255551,"user_tz":-60,"elapsed":22,"user":{"displayName":"Edoardo Maines","userId":"11184902831171502919"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["# Use the CUDA variable below to decide if training on GPU or CPU\n","# Note that all my experiments ran on GPU\n","\n","cuda = True\n","\n","if torch.cuda.is_available() and cuda:\n","    print(\"Model will be training on the GPU.\\n\")\n","    pass\n","    \n","else:\n","    cuda = False\n","    print(\"Model will be training on the CPU.\\n\")\n","\n","torch.cuda.manual_seed(5) if cuda else torch.manual_seed(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MTj1YbSyEiR3","outputId":"5cc7aa5d-77f3-4946-bf6f-374581181826","executionInfo":{"status":"ok","timestamp":1675422255551,"user_tz":-60,"elapsed":21,"user":{"displayName":"Edoardo Maines","userId":"11184902831171502919"}}},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["Model will be training on the GPU.\n","\n"]}]},{"cell_type":"code","source":["# decide batch size and sequence length to feed to the model\n","\n","seq_len = 35 \n","batch_size = 20 \n","\n","if cuda:\n","    print(\"Loading data on GPU...\")\n","    train_data = encoder.batch_encode(train).cuda()\n","    val_data = encoder.batch_encode(val).cuda()\n","    test_data = encoder.batch_encode(test).cuda()\n","else:\n","    print(\"Loading data on CPU...\")\n","    train_data = encoder.batch_encode(train)\n","    val_data = encoder.batch_encode(val)\n","    test_data = encoder.batch_encode(test)\n","\n","# (PyTorch-NLP) 'BPTTBatchSampler' samples sequentially a batch of source and target slices of size bptt_length.\n","# Typically, such a sampler, is used for language modeling training with backpropagation through time (BPTT).\n","\n","train_source_sampler, val_source_sampler, test_source_sampler = tuple(\n","    [BPTTBatchSampler(data = d, bptt_length = seq_len, batch_size = batch_size, \n","                      drop_last = True, type_ = 'source') for d in (train, val, test)])\n","\n","train_target_sampler, val_target_sampler, test_target_sampler = tuple(\n","    [BPTTBatchSampler(data = d, bptt_length = seq_len, batch_size = batch_size, \n","                      drop_last = True, type_ = 'target') for d in (train, val, test)])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"69kcdvAwFtXL","outputId":"4d4deef6-1827-49ac-fb2c-faccf54eec09","executionInfo":{"status":"ok","timestamp":1675422263992,"user_tz":-60,"elapsed":8456,"user":{"displayName":"Edoardo Maines","userId":"11184902831171502919"}}},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading data on GPU...\n"]}]},{"cell_type":"markdown","source":["Here decide if:\n","- you what to train the model(s) from scratch or just load them:\n","  - **train = True** --> from scratch;\n","  - **train = False** --> load the model.\n","- If training from scratch is chosen:\n","  - how many times to train the chosen model\n","  - which type of model to train:\n","    - **model_type = 0**: non-regularized LSTM;\n","    - **model_type = 1**: medium regularized LSTM;\n","    - **model_type = 2**: large regularized LSTM;\n","    - **model_type = 3**: variational LSTM."],"metadata":{"id":"q7ga4Ajgv6nD"}},{"cell_type":"code","source":["train = False\n","nruns = 5\n","ppls = np.zeros(nruns)\n","\n","# model_type:\n","# 0 --> non-regularized LSTM\n","# 1 --> medium regularized LSTM\n","# 2 --> large regularized LSTM\n","# 3 --> variational LSTM\n","\n","model_type = 3\n","\n","\n","ntokens = encoder.vocab_size\n","\n","if model_type == 0:\n","  beg = \"noREG\"\n","\n","  best_of_bests = 0\n","  \n","  emb_size = 200 \n","  hidden_size = 200 \n","  n_layers = 2 \n","  dropout = 0.0 \n","  init_weight = 0.1 \n","  max_norm = 5\n","  factor = 0.5\n","  init_lr = 1 \n","  tot_epochs = 13 \n","  epoch_decay_lr = 4 \n","  report_iterations = 500 \n","\n","elif model_type == 1:\n","  beg = \"medREG\"\n","\n","  best_of_bests = 2\n","\n","  emb_size = 650 \n","  hidden_size = 650 \n","  n_layers = 2\n","  dropout = 0.5 \n","  init_weight = 0.05 \n","  max_norm = 5 \n","  factor = 0.83333\n","  init_lr = 1 \n","  tot_epochs = 39 \n","  epoch_decay_lr = 6 \n","  report_iterations = 500 \n","\n","elif model_type == 2:\n","  beg = \"largeREG\"\n","\n","  best_of_bests = 0\n","  \n","  emb_size = 1500 \n","  hidden_size = 1500 \n","  n_layers = 2 \n","  dropout = 0.65 \n","  init_weight = 0.04 \n","  max_norm = 10\n","  factor = 0.8695652\n","  init_lr = 1 \n","  tot_epochs = 55 \n","  epoch_decay_lr = 14\n","  report_iterations = 500\n","\n","else:\n","  beg = \"varREG\"\n","\n","  best_of_bests = 3\n","\n","  emb_size = 650 \n","  hidden_size = 650 \n","  n_layers = 2\n","  dropout = 0.5\n","  init_weight = 0.05 \n","  max_norm = 5 \n","  factor = 0.83333\n","  init_lr = 1 \n","  tot_epochs = 39 \n","  epoch_decay_lr = 6\n","  report_iterations = 500\n","\n","  dropout_embedding=0.5 \n","  dropout_lstm_hidden=0.3 \n","  dropout_lstm_layer=0.4 \n","  dropout_output=0.5\n","\n","# initialize the model\n","if model_type != 3:\n","  model = Model(ntokens, emb_size, hidden_size, n_layers, init_weight, dropout)\n","else:\n","  model = VarModel(ntokens, emb_size, hidden_size, n_layers, init_weight, dropout_embedding, dropout_lstm_hidden, dropout_lstm_layer, dropout_output)\n","\n","if cuda:\n","  model.cuda()\n","\n","  \n","\n","# initialize the optimizer, scheduler and criterion for the loss\n","optimizer = torch.optim.SGD(model.parameters(), lr= 1, momentum=0.8)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = 1, gamma = factor) \n","criterion = nn.CrossEntropyLoss()"],"metadata":{"id":"G9Luioe4GUXB","executionInfo":{"status":"ok","timestamp":1675422439414,"user_tz":-60,"elapsed":1053,"user":{"displayName":"Edoardo Maines","userId":"11184902831171502919"}}},"execution_count":61,"outputs":[]},{"cell_type":"code","source":["if train:\n","  with open(beg+'out.txt', 'w') as f: \n","\n","      print(\"PARAMETERS\", file = f)\n","\n","      print(\"\\nntokens = {:d}\\nemb_size = {:d}\\nhidden_size = {:d}\\nn_layers = {:d}\\ndropout = {:.3f}\\ninit_weight = {:.3f}\\nmax_norm = {:d}\\nfactor = {:.3f}\\ninit_lr = {:d}\\ntot_epochs = {:d}\\nepoch_decay_lr = {:d}\\nreport_iterations = {:d}\".format(ntokens, emb_size, hidden_size, n_layers,\n","                                                  dropout, init_weight, max_norm, factor,\n","                                                  init_lr, tot_epochs, epoch_decay_lr,\n","                                                  report_iterations), file = f)      \n","      print(model, file = f)\n","\n","      for run in tqdm(range(nruns)):\n","\n","          print(\"\\n\\nRUN {:d}\\n\\n\".format(run+1), file = f)                   \n","          best_val_loss = []\n","          stored_loss = math.inf\n","\n","          path_save_model = \"./trained_models/\"+beg\n","          \n","          for epoch in tqdm(range(0, tot_epochs)):\n","              \n","              print(\"\\n##################################################################################\\n\", file = f)\n","              print(\"EPOCH: \", epoch+1, file = f)\n","              print(\"\\nTrain ------------\\n\", file = f)\n","              train_step()\n","              print(\"\\n\\nValidation -------\\n\", file = f)\n","              val_avg_loss, val_avg_ppl = test_step(val_data, val_source_sampler, val_target_sampler)\n","              print(\"\\nAvg loss: {:.3f} | Avg ppl: {:.3f}\\n\".format(val_avg_loss, val_avg_ppl), file = f)\n","              if val_avg_loss < stored_loss:\n","                  torch.save(model.state_dict(), path_save_model+str(run)+\".pt\")\n","                  print('\\nSaving model (new best validation)', file = f)\n","                  stored_loss = val_avg_loss\n","\n","\n","          print(\"\\n\\n\\n\\n\\nLoading the best model...\\n\", file = f)\n","          model.load_state_dict(torch.load(path_save_model+str(run)+\".pt\"))\n","          \n","        \n","          print(\"*\"*89, file = f)\n","          print(\"*\"*89, file = f)\n","          print(\"\\nTESTING ----------\", file = f)\n","          test_avg_loss, test_avg_ppl = test_step(test_data, test_source_sampler, test_target_sampler)\n","          print(\"Avg loss: {:.3f} | Avg ppl: {:.3f}\\n\".format(test_avg_loss, test_avg_ppl), file = f)\n","          print(\"*\"*89, file = f)\n","          print(\"*\"*89, file = f)\n","\n","          ppls[run] = test_avg_ppl\n","\n","      best_of_bests = ppls.argmin()\n","      print(\"\\n\\nFinal average ppl over {:d} runs is {:.3f} with a standard deviation of {:.3f}\".format(nruns, np.mean(ppls), np.std(ppls)), file = f)\n","\n","      print(\"\\nBest final model was the one in run \", best_of_bests, file = f)\n","\n","      f.close()\n","\n","else:\n","  path_load_model = \"./trained_models/\"+beg+str(best_of_bests)+\".pt\"\n","\n","  print(\"\\nLoading the best model...\\n\")\n","  model.load_state_dict(torch.load(path_load_model))\n","\n","  print(\"\\nTESTING ----------\")\n","  test_avg_loss, test_avg_ppl = test_step(test_data, test_source_sampler, test_target_sampler)\n","  print(\"Avg loss: {:.3f} | Avg ppl: {:.3f}\\n\".format(test_avg_loss, test_avg_ppl))"],"metadata":{"id":"UJMK2x9d780A","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b0f3f73c-c905-43f0-ae9d-16e3a6f5e2d5","executionInfo":{"status":"ok","timestamp":1675422447418,"user_tz":-60,"elapsed":4333,"user":{"displayName":"Edoardo Maines","userId":"11184902831171502919"}}},"execution_count":62,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Loading the best model...\n","\n","\n","TESTING ----------\n","Avg loss: 4.435 | Avg ppl: 84.338\n","\n"]}]},{"cell_type":"markdown","source":["## **Predict**"],"metadata":{"id":"ObKlCjWiwg1o"}},{"cell_type":"code","source":["# given a initial sentence and a number of words to generate this function will use\n","# the last loaded model to predict the words\n","def predict_word(start_text, nwords2gen):\n","    start_text_processed = []\n","    for word in start_text:\n","      if word in encoder.vocab:\n","        start_text_processed.append(word)\n","      else:\n","        start_text_processed.append(\"<unk>\")    \n","    \n","    new_words = torch.cuda.LongTensor()\n","    model.eval()\n","    with torch.no_grad():\n","        source = encoder.batch_encode(start_text_processed).cuda()\n","        source = torch.reshape(source, (list(source.size())[0], 1))\n","\n","        hidden = model.init_hidden(1)\n","        soft = nn.Softmax(dim = 1)\n","        for i in range(nwords2gen):\n","          data4model = source\n","\n","          output, hidden = model(data4model, hidden)\n","\n","          next_word_probs = soft(output)[-1]\n","          \n","          get_word_idx = torch.multinomial(next_word_probs, 1)\n","          if get_word_idx == encoder.encode(\"<unk>\") or get_word_idx == encoder.encode(\"N\") or get_word_idx == encoder.encode(\"$\"):\n","            next_word_probs[get_word_idx] = 0.\n","            get_word_idx = torch.multinomial(next_word_probs, 1)\n","\n","          new_words = torch.cat((new_words, get_word_idx))\n","          get_word_idx = get_word_idx[-1:,None]\n","      \n","          source = torch.cat((source, get_word_idx), dim = 0)\n","\n","          to_print = []\n","          for w in start_text+encoder.batch_decode(new_words):\n","            if w == \"</s>\":\n","              to_print.append(\".\")\n","            else:\n","              to_print.append(w)\n","          print(\" \".join(to_print))        "],"metadata":{"id":"qaJ5IED9QzAZ","executionInfo":{"status":"ok","timestamp":1675422269283,"user_tz":-60,"elapsed":23,"user":{"displayName":"Edoardo Maines","userId":"11184902831171502919"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["how_many_next_words = 10\n","plain_txt = \"The meaning of life is\"\n","\n","split_text = plain_txt.split()\n","predict_word(split_text, how_many_next_words)"],"metadata":{"id":"QEztgDzGQ2pS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675422473605,"user_tz":-60,"elapsed":555,"user":{"displayName":"Edoardo Maines","userId":"11184902831171502919"}},"outputId":"86550a6e-a7ad-4263-9755-1f2ce53d389c"},"execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":["The meaning of life is stiff\n","The meaning of life is stiff for\n","The meaning of life is stiff for the\n","The meaning of life is stiff for the other\n","The meaning of life is stiff for the other types\n","The meaning of life is stiff for the other types of\n","The meaning of life is stiff for the other types of discipline\n","The meaning of life is stiff for the other types of discipline and\n","The meaning of life is stiff for the other types of discipline and the\n","The meaning of life is stiff for the other types of discipline and the otherwise\n"]}]}]}